{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea977f7-33fb-4f4a-8659-45fe60ede4b2",
   "metadata": {},
   "source": [
    "The more AI is used the more opportunity there is for catastrophic failure. We’ve already seen many failures in the short time that foundation models have been around. A man committed suicide after being encouraged by a chatbot. Lawyers sub‐ mitted false evidence hallucinated by AI. Air Canada was ordered to pay damages when its AI chatbot gave a passenger false information. \n",
    "Without a way to quality control AI outputs, the risk of AI might outweigh its benefits for many applications.\n",
    "\n",
    "### The Fundamental Challenges of Foundation Model Evaluation\n",
    "\n",
    "\n",
    "Verification Complexity: \n",
    "\n",
    "\n",
    "Evaluating ML models has always been difficult. With the introduction of foundation models, evaluation has become even more so. There are multiple reasons why evaluating foundation models is more challenging than evaluating traditional ML models. \n",
    "\n",
    "1. Evaluation Now Requires Human-Level Analysis\n",
    "Traditional ML evaluation often relied on simple metrics - is the classification correct? Does the prediction match the expected value? With foundation models, we're dealing with tasks that require human-level understanding to evaluate. Imagine checking a legal document summary - you need to:\n",
    "- Read and understand the original document\n",
    "- Verify all factual claims, assess logical consistency, and check for subtle misrepresentations\n",
    "This isn't just more time-consuming - it's an entirely different kind of evaluation that often requires domain expertise.\n",
    "\n",
    "2. There might not be a Single \"Right\" Answer\n",
    "Traditional ML models operate in closed worlds - a cat photo should be classified as \"cat,\" an email as \"spam\" or \"not spam.\" Foundation models operate in an open world where: multiple answers can be equally valid, quality of an answer often depends on context, and there's no comprehensive list of \"correct\" responses\n",
    "- open-ended nature of foundation models undermines the traditional approach of evaluating models against ground truths\n",
    "- With traditional ML, most tasks are closed-ended. Classification model can only output among the expected categories. To evaluate a classification model, you can evaluate its outputs against expected outputs. If the expected output is in Category X, but the model's output is in Category Y, the model is wrong. \n",
    "\n",
    "3. The Models Are Black Boxes\n",
    "Most foundation models are effectively black boxes, either because (1) Providers don't share internal details (2) The models are too complex for most users to understand.\n",
    "- Most foundation models are treated as black boxes either because model providers choose not to expose model details or because the model users lack the expertise to understand them.\n",
    "- Details such as the model architecture, training data, and training process can reveal a lot about the model's strengths and weaknesses. Without those details, you can only evaluate a model by observing its output.\n",
    "\n",
    "\n",
    "4.  We Don't Even Know What They Can Do\n",
    "With traditional ML, you knew what you were evaluating - a model trained to detect cats should detect cats. Foundation models can develop unexpected capabilities, solve problems they weren't explicitly trained for, sometimes exceed human capabilities in specific tasks.\n",
    "- The scope of evaluation is expanded for general-purpose models. With task-specific models, evaluation involves measuring a model's performance on its training task. However, with general-purpose models, evaluation is not only about assessing a model's performance on known tasks, but also about discovering new tasks that the model can do. These might include tasks that extend beyond human capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca24c01-c187-4572-b0e8-d588f4a7d68c",
   "metadata": {},
   "source": [
    "### The Fundamental Challenges of Foundation Model Evaluation\n",
    "\n",
    "**Verification Complexity:**\n",
    "\n",
    "- Traditional ML evaluation often involves simple output matching, but foundation model outputs require multi-faceted verification\n",
    "- Evaluating outputs requires multi-dimensional analysis (factual accuracy, safety, coherence)\n",
    "- The cost and time for thorough evaluation increases exponentially with task complexity\n",
    "- Traditional automated metrics fail to capture true performance\n",
    "\n",
    "**Absence of Ground Truth**\n",
    "- Foundation models operate in open-ended solution spaces where multiple valid outputs exist\n",
    "- Traditional evaluation metrics based on exact matching or categorical accuracy break down\n",
    "- The space of \"correct\" answers is often infinite and context-dependent\n",
    "- Determining output quality becomes subjective and nuanced; Quality exists on a spectrum rather than binary correct/incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5fe89-d6ae-4ccc-91a8-6a64200f7e14",
   "metadata": {},
   "source": [
    "**Entropy** (H) measures the average amount of information contained in a random variable or probability distribution. \n",
    "\n",
    "**Cross-entropy $H(P,Q)$ measures the average number bits needed to encode data from a true distribution $P$ using a predicted distribution $Q$\n",
    "\n",
    "When we train a model on a dataset, our goal is to get the model to learn the distribution of this training data. In other words, our goal is to get the model to predict what comes next in the training data. \n",
    "\n",
    "A model's cross entropy on the training data depends on two qualities:\n",
    "1. The training data's predictability, measured by the training data's entropy\n",
    "2. How the distribution captured by the langauge model diverges from the true distribution of the training data.\n",
    "\n",
    "Let $P$ be the true distribution of the training data, and $Q$ be the distribution learned by the langauge.\n",
    "- the training data's entropy is therefore $H(P)$\n",
    "- the divergence of $Q$ with respect to $P$ can be measured using Killback-Liebler (KL) divergence, which is mathematically represented as $D_{KL}(P||Q)$\n",
    "\n",
    "---\n",
    "\n",
    "## Bits-per-Character and Bits-per-Byte\n",
    "\n",
    "One unit of entropy and cross entropy is bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658614e-f918-4d16-b5f4-80f0ddbf77ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
