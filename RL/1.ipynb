{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781358e2-bfd6-4c89-b17a-b54a8f95b8e5",
   "metadata": {},
   "source": [
    "# 1. Prediction and Control\n",
    "\n",
    "Now that we’ve covered basic probability, let’s dive into a simple reinforcement learning scenario: a gambling game. This example will introduce core RL ideas of prediction (evaluating how good a given strategy is) and control (finding a better strategy). We will define a simple gambling environment and simulate an agent’s decisions within it.\n",
    "\n",
    "### Problem Setup: A Gambler’s Game\n",
    "Imagine a gambler who starts with a certain amount of money and aims to reach a target amount. At each turn, the gambler can bet some of their money on a coin flip:\n",
    "- If the coin comes up Heads (win), the gambler wins the amount bet (it gets added to their capital).\n",
    "- If Tails (lose), the gambler loses the amount bet (subtracted from their capital).\n",
    "- The coin might be biased with probability p of Heads.\n",
    "\n",
    "The game ends either when the gambler reaches the target (goal) or loses all money (bust).\n",
    "\n",
    "\n",
    "This is known as the Gambler’s Problem from Sutton and Barto’s RL textbook. \n",
    "- States: The gambler’s capital (from $0$ to $Goal$).\n",
    "- Actions: The bet amount (from $1$ up to the smaller of current capital or amount needed to reach goal).\n",
    "- Transition: Win or lose based on the coin flip.\n",
    "- Reward: We can define a reward of +1 for reaching the goal, and 0 otherwise (no reward each step except at the end). This way, maximizing total reward is equivalent to maximizing the probability of reaching the goal.\n",
    "\n",
    "For simplicity, let’s implement a smaller version: goal = $10, starting capital = $5, and a fair coin (p = 0.5). The gambler can bet $1 each turn (to keep it simple initially).\n",
    "\n",
    "\n",
    "#### Simulation of a Fixed Policy\n",
    "\n",
    "Policy: Let’s start with a simple policy: always bet $1 until the game ends.\n",
    "\n",
    "\n",
    "We will simulate many episodes (games) with this policy to estimate:\n",
    "- Probability of reaching the goal (this is the value of the starting state under this policy).\n",
    "- The distribution of episode lengths (how many bets until finish).\n",
    "\n",
    "Let’s write a simulation for one episode and then loop it for many episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b3482c-76ed-4048-9d30-43f67b4b3f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode result: Broke :(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def simulate_gamble(start_capital, goal, p_head=0.5):\n",
    "    \"\"\"\n",
    "    Simulate one episode of the gambling game.\n",
    "    Returns True if goal reached (win), False if gambler goes broke (lose).\n",
    "    Policy: always bet 1.\n",
    "    \"\"\"\n",
    "    capital = start_capital\n",
    "    while 0 < capital < goal:\n",
    "        # Always bet 1\n",
    "        bet = 1\n",
    "        if random.random() < p_head:  # win\n",
    "            capital += bet\n",
    "        else:  # lose\n",
    "            capital -= bet\n",
    "        # No intermediate rewards in this formulation\n",
    "    # Loop ends when capital == 0 or capital == goal\n",
    "    return capital >= goal  # True if reached goal, False if busted\n",
    "\n",
    "# Test one episode\n",
    "result = simulate_gamble(5, 10, p_head=0.5)\n",
    "print(\"Episode result:\", \"Reached goal!\" if result else \"Broke :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7de5fb-c401-410f-9442-7c5d2a3c20bc",
   "metadata": {},
   "source": [
    "Run the above cell a few times to see different outcomes (sometimes the gambler wins, sometimes loses).\n",
    "\n",
    "Now, let’s estimate the probability of winning from the starting capital of 5 with this policy by simulating many episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c228ea-f80d-46f7-a2b6-6c64a26e1395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of reaching $10 starting from $5 (bet $1 policy) = 0.498\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "trials = 10000\n",
    "wins = 0\n",
    "for _ in range(trials):\n",
    "    if simulate_gamble(5, 10, p_head=0.5):\n",
    "        wins += 1\n",
    "win_prob = wins / trials\n",
    "print(f\"Estimated probability of reaching $10 starting from $5 (bet $1 policy) = {win_prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f63e693-c007-44dd-a454-23ebfcd7aabc",
   "metadata": {},
   "source": [
    "With a fair coin and symmetric game, you might expect ~50% chance to reach the goal when starting at half the goal (in fact, for an unbiased random walk, the probability of hitting +5 before -5 starting at 0 is 50%). Our simulation likely shows close to 0.5.\n",
    "\n",
    "\n",
    "This probability is essentially the value of the state “capital=5” under the current policy (always bet 1). In prediction terms, we’ve estimated $v_{\\pi}(5)$ for our policy $\\pi$ (always bet 1) as ~0.5.\n",
    "\n",
    "\n",
    "We can similarly estimate $v_{\\pi}(s)$ for other starting states s by changing `start_capital`.\n",
    "\n",
    "**Exercise**: Try simulating from start_capital=1 and start_capital=9 to see the win probabilities. (We expect $v_{\\pi}(1)$ to be much lower than $v_{\\pi}(9)$, since with only $\\$1$ the gambler is one loss from ruin, whereas with $\\$9$ they’re one step from the goal.)\n",
    "\n",
    "### Prediction: State Value Function\n",
    "\n",
    "For this gambling game, the state value $v_{\\pi}(s)$ is the probability of reaching the goal from state $s$ under policy $\\pi (because we set reward 1 for success, 0 otherwise, so the expected return equals the success probability).\n",
    "\n",
    "\n",
    "We can approximate the entire value function for our policy by simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a7d3f0-2115-49f4-9d24-91c950a051c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated value of each state (always bet 1 policy):\n",
      "v(0) ≈ 0.00\n",
      "v(1) ≈ 0.10\n",
      "v(2) ≈ 0.19\n",
      "v(3) ≈ 0.31\n",
      "v(4) ≈ 0.40\n",
      "v(5) ≈ 0.50\n",
      "v(6) ≈ 0.58\n",
      "v(7) ≈ 0.71\n",
      "v(8) ≈ 0.80\n",
      "v(9) ≈ 0.91\n",
      "v(10) ≈ 1.00\n"
     ]
    }
   ],
   "source": [
    "def estimate_values(policy_func, goal, p_head=0.5, trials=10000):\n",
    "    \"\"\"\n",
    "    Estimate value of each capital state 0..goal for a given policy by simulation.\n",
    "    policy_func(state) -> bet amount\n",
    "    Returns a dict of state -> estimated value.\n",
    "    \"\"\"\n",
    "    values = {s: 0.0 for s in range(goal+1)}\n",
    "    for s in range(1, goal):  # exclude terminal states 0 and goal\n",
    "        wins = 0\n",
    "        for _ in range(trials):\n",
    "            # simulate from state s\n",
    "            capital = s\n",
    "            while 0 < capital < goal:\n",
    "                bet = policy_func(capital)\n",
    "                if random.random() < p_head:\n",
    "                    capital += bet\n",
    "                else:\n",
    "                    capital -= bet\n",
    "            if capital >= goal:\n",
    "                wins += 1\n",
    "        values[s] = wins / trials\n",
    "    values[0] = 0.0\n",
    "    values[goal] = 1.0\n",
    "    return values\n",
    "\n",
    "# Define our always-bet-1 policy function\n",
    "policy_bet1 = lambda capital: 1\n",
    "\n",
    "val_estimates = estimate_values(policy_bet1, goal=10, p_head=0.5, trials=2000)\n",
    "print(\"Estimated value of each state (always bet 1 policy):\")\n",
    "for s in range(0, 11):\n",
    "    print(f\"v({s}) ≈ {val_estimates[s]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac900e6-876c-44a4-a778-2b782d3f6b9f",
   "metadata": {},
   "source": [
    "You should see the values increasing as the capital gets higher (closer to the goal). State 0 has value 0 (game lost) and state 10 has value 1 (goal achieved). For a fair coin, the value might roughly increase linearly with the capital (since each $\\$1$ is like one step closer to victory on average). In fact, for the fair coin random walk, the true $v_{\\pi}(s)$ should be $s/10$ in this scenario (starting with $s$ dollars out of 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a3581-48af-47f7-a728-530f9b987b16",
   "metadata": {},
   "source": [
    "### Control: Comparing Different Policies\n",
    "Now we have a way to evaluate a given policy (by simulation). The next step is control: finding a better policy.\n",
    "Let’s compare two policies:\n",
    "- $\\pi_1$: Always bet 1 (we evaluated this).\n",
    "- $\\pi_2$: Always bet all-in (bet all current capital each time).\n",
    "\n",
    "Intuition:\n",
    "- $\\pi_2$ is riskier but reaches the goal faster if it wins. For a fair coin, any bet size yields the same success probability in theory (actually for p=0.5, any strategy gives $s/Goal$ chance). But for a biased coin, the optimal strategy changes.\n",
    "- Let’s test with a biased coin, p = 0.4 (40% chance to win each bet, so the odds are against the gambler). In that case, a high-variance strategy (betting big) might increase success chances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5c1d48-9c69-4eef-8c9f-ab28b58d44ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy π1 (bet 1) win probability ≈ 0.116\n",
      "Policy π2 (all-in) win probability ≈ 0.392\n"
     ]
    }
   ],
   "source": [
    "def simulate_policy(start_capital, goal, p_head, policy_func):\n",
    "    capital = start_capital\n",
    "    while 0 < capital < goal:\n",
    "        bet = policy_func(capital)\n",
    "        if random.random() < p_head:\n",
    "            capital += bet\n",
    "        else:\n",
    "            capital -= bet\n",
    "    return capital >= goal\n",
    "\n",
    "def estimate_win_prob(start_capital, goal, p_head, policy_func, trials=5000):\n",
    "    wins = 0\n",
    "    for _ in range(trials):\n",
    "        if simulate_policy(start_capital, goal, p_head, policy_func):\n",
    "            wins += 1\n",
    "    return wins / trials\n",
    "\n",
    "# Define policy functions:\n",
    "policy_all_in = lambda capital: capital  # bet all current capital\n",
    "\n",
    "# Estimate win probabilities for start=5, goal=10, p=0.4\n",
    "win_prob_bet1 = estimate_win_prob(5, 10, p_head=0.4, policy_func=policy_bet1, trials=10000)\n",
    "win_prob_allin = estimate_win_prob(5, 10, p_head=0.4, policy_func=policy_all_in, trials=10000)\n",
    "print(f\"Policy π1 (bet 1) win probability ≈ {win_prob_bet1:.3f}\")\n",
    "print(f\"Policy π2 (all-in) win probability ≈ {win_prob_allin:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd67635-ef52-44c3-843b-adcbf521ec22",
   "metadata": {},
   "source": [
    "It’s likely that the all-in policy has a higher win probability when p < 0.5. For example, starting with $\\$5$, goal $10, p=0.4$:\n",
    "- Bet-1 policy requires winning 5 times before losing 5 times (very low probability).\n",
    "- All-in policy needs one win of 5 (0.4 chance).\n",
    "So $\\pi_2$ might show ~0.4 win prob, whereas $\\pi_1$ could be much lower (the gambler will probably bust most of the time due to needing consecutive wins).\n",
    "\n",
    "**Result Discussion**: The better policy is the one with higher win probability (or higher expected reward). In this case, all-in seems better for p=0.4. If p were 0.6 (favorable odds), the opposite would be true: betting small repeatedly would yield a higher overall chance to eventually accumulate wins (because the odds are in your favor, you want many small bets to reduce variance).\n",
    "This simple comparison illustrates policy improvement: if we find a policy that yields higher value for the starting state, it’s a better policy. In reinforcement learning, we often iteratively improve a policy by looking at such comparisons.\n",
    "\n",
    "This simple comparison illustrates policy improvement: if we find a policy that yields higher value for the starting state, it’s a better policy. In reinforcement learning, we often iteratively improve a policy by looking at such comparisons.\n",
    "\n",
    "\n",
    "\n",
    "### Toward Optimal Policy\n",
    "What is the optimal policy for the gambler? We won’t derive it fully here (that’s an exercise for Dynamic Programming later), but intuitively:\n",
    "- If the coin is unfair (p < 0.5), optimal strategy is to take risky bets (high variance) to maximize the chance of a lucky streak\n",
    "- If the coin is in your favor (p > 0.5), optimal is to bet small increments (low variance) to steadily accumulate wins\n",
    "- If $p = 0.5$ exactly, many strategies yield the same 50% success probability, so the “optimal” is not unique.\n",
    "\n",
    "\n",
    "Next, we’ll explore a different classic RL problem: the multi-armed bandit, which focuses on the exploration-exploitation trade-off in a simpler setting (no multiple states, just choosing actions for immediate reward)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
