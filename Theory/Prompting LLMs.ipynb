{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16cffdc8-b91f-497d-b43f-71323c2df975",
   "metadata": {},
   "source": [
    "# Neural Sequence Models and Pre-training\n",
    "\n",
    "Transformer architectures [1] and advances in self-supervised learning have fundamentally altered natural language processing. By pre-training neural networks on vast unlabeled datasets using self-supervision, we can create foundation models that adapt to diverse tasks through fine-tuning or prompting. This approach often eliminates the need for task-specific supervised learning, instead relying on the adaptation of pre-trained models.\n",
    "\n",
    "$$\n",
    "\\text{Foundation Model} = \\text{Architecture} + \\text{Self-Supervised Pre-training}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The significance of large language models lies in their emergent properties: while trained solely on next-token prediction ($P(x_t|x_{<t})$), they demonstrate sophisticated linguistic understanding and generalization across diverse NLP tasks, often surpassing specialized supervised systems. Recent research [2] suggests these emergent capabilities may extend beyond purely linguistic domains.\n",
    "\n",
    "# Prompting of Pre-trained Models\n",
    "Large language models employ next-token prediction ($P(x_t|x_{<t})$) at scale to develop general language understanding. Through extensive training, this seemingly simple objective enables these models to transform complex NLP tasks into text generation problems via prompting. This capability allows for reformulating traditional tasks, such as classification, into generation tasks without task-specific architectures.\n",
    "\n",
    "\n",
    "### Prompting for Text Classification\n",
    "\n",
    "Consider how humans classify text - we read a passage and naturally understand its sentiment or category. Modern language models can perform a similar task, but instead of making a direct classification, they complete a carefully crafted prompt. This approach transforms the traditional classification problem into a more natural language generation task.\n",
    "Let's look at a concrete example:\n",
    "```\n",
    "Assume that the polarity of a text is a label chosen from {positive, negative, neutral}. Identify the polarity of the input.\n",
    "Input: I love the food here. It’s amazing!\n",
    "Polarity: ________\n",
    "```\n",
    "When presented with this prompt, the model completes the blank with a classification label, much like how a human would naturally respond.\n",
    "\n",
    "\n",
    "To formalize this intuition, we can express the classification process mathematically. For any input text $x$, we create a prompt template $p(x)$ and predict the most likely completion:\n",
    "\n",
    "$$\n",
    "\\text{class}(x) = f(\\text{argmax}_{y \\in V} P(y|p(x)))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $V$ is the model's vocabulary (all possible tokens it might generate)\n",
    "- $y$ is the predicted completion token\n",
    "- $P(y|p(x))$ is the probability of generating token $y$ given the prompt\n",
    "- $f$ is a mapping function that converts completions to formal class labels\n",
    "\n",
    "\n",
    "\n",
    "An instruction-based prompt consists of three key components:\n",
    "\n",
    "1. **Task Instructions**\n",
    "   ```\n",
    "   Assume that the polarity of a text is a label chosen from {positive, negative, neutral}.\n",
    "   Identify the polarity of the input.\n",
    "   ```\n",
    "   This defines the task parameters and expected output format.\n",
    "\n",
    "2. **Format Markers**\n",
    "   ```\n",
    "   Input: [text goes here]\n",
    "   Polarity: [model completion here]\n",
    "   ```\n",
    "   These markers provide structural cues that:\n",
    "   - Clearly separate different components of the prompt\n",
    "   - Guide the model's attention to relevant sections\n",
    "   - Structure the expected response format\n",
    "\n",
    "3. **Input Text**\n",
    "   ```\n",
    "   I love the food here. It's amazing!\n",
    "   ```\n",
    "\n",
    "The complete prompt combines these elements:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(x) &= \\text{instructions} \\\\\n",
    "&+ \\text{\"Input: \"} + x \\\\\n",
    "&+ \\text{\"Polarity: \"}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 30, 5998-6008.\n",
    "\n",
    "\n",
    "[2] Bubeck, S., et al. (2023). Sparks of artificial general intelligence: Early experiments with GPT-4.\n",
    "\n",
    "\n",
    "\n",
    "Brown, T. B., et al. (2020). \"Language Models are Few-Shot Learners.\" arXiv preprint arXiv:2005.14165.\n",
    "\n",
    "\n",
    "Wei, J., et al. (2022). \"Chain of Thought Prompting Elicits Reasoning in Large Language Models.\" arXiv preprint arXiv:2201.11903.\n",
    "\n",
    "\n",
    "Kojima, T., et al. (2022). \"Large Language Models are Zero-Shot Reasoners.\" arXiv preprint arXiv:2205.11916."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04df215-fca1-4b6f-a644-04c6962cd901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
