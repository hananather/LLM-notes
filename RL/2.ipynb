{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b1ae07-14f7-47eb-89c6-4c59c51b46ee",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits\n",
    "\n",
    "Multi-armed bandits (MABs) are a simpler class of problems that capture the fundamental challenge of exploration vs. exploitation in reinforcement learning. The scenario is like a gambler in front of several slot machines (“one-armed bandits”): each pull of a lever gives a stochastic reward. The gambler (agent) wants to maximize reward over time by choosing which machines to play, balancing trying new machines (exploration) and sticking with the best one found so far (exploitation).\n",
    "\n",
    "# Problem Setup\n",
    "- You have K slot machines (arms). Each arm, when pulled, gives a reward drawn from a fixed distribution (unknown to you). For simplicity, let’s say each arm gives a reward of 1 with some probability (and 0 otherwise) – a Bernoulli bandit.\n",
    "- Your goal is to maximize the total reward over a series of pulls (trials).\n",
    "\n",
    "Key challenge: Exploration vs. Exploitation\n",
    "- Exploration: Try different arms to gain information about their payoff.\n",
    "- Exploitation: Use the information to pick the arm with the highest known reward rate so far.\n",
    "\n",
    "\n",
    "We will implement and compare three strategies:\n",
    "1. $\\epsilon$-Greedy – with probability ε, explore (random arm), otherwise exploit (best arm)\n",
    "2. Upper Confidence Bound (UCB) – select the arm with the highest upper confidence bound on reward (favoring arms with less trials so far to explore uncertainty\n",
    "3. Thompson Sampling – a Bayesian approach: maintain a distribution for each arm’s success probability and sample from these to decide (naturally balancing exploration and exploitation)\n",
    "\n",
    "Let’s set up a bandit problem to test these. We will create a bandit with, say, 3 arms:\n",
    "- Arm 0: win probability 0.3\n",
    "- Arm 1: win probability 0.5\n",
    "- Arm 2: win probability 0.7\n",
    "\n",
    "The best arm is #2 (70% chance of reward), but the agents don’t know that initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af49552f-f959-4c9b-8110-112241caa124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# True probabilities for each arm (unknown to agent)\n",
    "true_probs = [0.3, 0.5, 0.7]\n",
    "K = len(true_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f025500-78c0-4667-b6da-5c44374426b6",
   "metadata": {},
   "source": [
    "We’ll simulate a sequence of 1000 pulls. At each step, the agent chooses an arm according to the strategy, and gets a reward 1 (with the arm’s true probability) or 0. We will track:\n",
    "- The cumulative reward over time.\n",
    "- The number of pulls of each arm (to see exploration)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c7d9f-745a-4edb-93c1-ac4e3f7d6bb0",
   "metadata": {},
   "source": [
    "### ε-Greedy Strategy\n",
    "\n",
    "The ε-greedy strategy is simple and widely used:\n",
    "- We maintain an estimate of each arm’s value (e.g., average reward observed).\n",
    "- On each step, with probability ε choose a random arm (explore), otherwise choose the arm with highest estimated value (exploit).\n",
    "- After getting the reward, update the arm’s value estimate (incremental average).\n",
    "\n",
    "Let’s implement ε-greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7535a560-2640-4677-a4b8-724a47e24f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-Greedy results:\n",
      "Arm counts: [2993, 3009, 83998]\n",
      "Estimated values: ['0.29', '0.49', '0.70']\n",
      "Total reward: 61195\n"
     ]
    }
   ],
   "source": [
    "def run_epsilon_greedy(true_probs, epsilon=0.1, trials=1000):\n",
    "    # Initialize estimates and counts\n",
    "    counts = [0] * K\n",
    "    values = [0.0] * K  # estimated value (mean reward) for each arm\n",
    "    rewards_history = []\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(1, trials+1):\n",
    "        # Decide arm\n",
    "        if random.random() < epsilon:\n",
    "            arm = random.randrange(K)  # explore\n",
    "        else:\n",
    "            # exploit (argmax of values, tie-break randomly)\n",
    "            max_val = max(values)\n",
    "            best_arms = [i for i, v in enumerate(values) if v == max_val]\n",
    "            arm = random.choice(best_arms)\n",
    "        # Pull arm\n",
    "        reward = 1 if random.random() < true_probs[arm] else 0\n",
    "        # Update counts and value estimate\n",
    "        counts[arm] += 1\n",
    "        # Incremental average update for mean:\n",
    "        values[arm] += (reward - values[arm]) / counts[arm]\n",
    "        # Track total reward\n",
    "        total_reward += reward\n",
    "        rewards_history.append(total_reward)\n",
    "    return rewards_history, counts, values\n",
    "\n",
    "# Run epsilon-greedy\n",
    "history_eps, counts_eps, values_eps = run_epsilon_greedy(true_probs, epsilon=0.1, trials=90000)\n",
    "print(\"Epsilon-Greedy results:\")\n",
    "print(\"Arm counts:\", counts_eps)\n",
    "print(\"Estimated values:\", [f\"{v:.2f}\" for v in values_eps])\n",
    "print(\"Total reward:\", history_eps[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2190f753-e831-4035-9b16-c97b6bb37037",
   "metadata": {},
   "source": [
    "**Question**: What happens if ε is too high or too low?\n",
    "Feel free to try different ε values in `run_epsilon_greedy` to see the effect on total reward.\n",
    "\n",
    "### Upper Confidence Bound (UCB) Strategy\n",
    "UCB is a more advanced strategy that addresses a limitation of ε-greedy: ε-greedy explores blindly. UCB uses a confidence interval approach: it picks the arm with the highest upper confidence bound on the estimated reward.One common formula (UCB1) for each arm a at time t is:\n",
    "$$\n",
    "\\text{UCB}_a = \\hat{Q}_a + \\sqrt{\\frac{2 \\ln t}{N_a}}\n",
    "$$\n",
    "where:\n",
    "- $\\hat{Q}_a$ is the current estimated value (mean reward) of arm a.\n",
    "- $N_a$ is how many times arm a has been pulled.\n",
    "- $N_a$ is how many times arm a has been pulled.\n",
    "\n",
    "We will implement UCB1. Note: We need to pull each arm at least once initially to get an estimate (and avoid division by zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed98226-4afd-4fbd-9da7-47eb2a56077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UCB results:\n",
      "Arm counts: [53, 146, 801]\n",
      "Estimated values: ['0.30', '0.51', '0.69']\n",
      "Total reward: 644\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def run_ucb(trials=1000):\n",
    "    counts = [0] * K\n",
    "    values = [0.0] * K \n",
    "    rewards_history = []\n",
    "    total_reward = 0\n",
    "\n",
    "    # Initial phase: play each arm once\n",
    "    for arm in range(K):\n",
    "        reward = 1 if random.random() < true_probs[arm] else 0\n",
    "        counts[arm] += 1\n",
    "        values[arm] += reward  # single sample mean is the reward itself\n",
    "        total_reward += reward\n",
    "        rewards_history.append(total_reward)\n",
    "    # Now do the remaining trials\n",
    "    for t in range(K+1, trials+1):\n",
    "        # Compute UCB for each arm\n",
    "        ucb_values = []\n",
    "        for arm in range(K):\n",
    "            # Exploitation term = values[arm]\n",
    "            # Exploration term = sqrt(2 * ln(t) / counts[arm])\n",
    "            exploration_bonus = math.sqrt(2 * math.log(t) / counts[arm])\n",
    "            ucb_values.append(values[arm] + exploration_bonus)\n",
    "        # Select arm with highest UCB\n",
    "        arm = ucb_values.index(max(ucb_values))\n",
    "        # Pull arm\n",
    "        reward = 1 if random.random() < true_probs[arm] else 0\n",
    "        counts[arm] += 1\n",
    "        # Update mean estimate\n",
    "        values[arm] += (reward - values[arm]) / counts[arm]\n",
    "        total_reward += reward\n",
    "        rewards_history.append(total_reward)\n",
    "    return rewards_history, counts, values\n",
    "\n",
    "history_ucb, counts_ucb, values_ucb = run_ucb(trials=1000)\n",
    "print(\"\\nUCB results:\")\n",
    "print(\"Arm counts:\", counts_ucb)\n",
    "print(\"Estimated values:\", [f\"{v:.2f}\" for v in values_ucb])\n",
    "print(\"Total reward:\", history_ucb[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e34d3-f882-42f7-b363-e67931b7ae5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
