{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b1ae07-14f7-47eb-89c6-4c59c51b46ee",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits\n",
    "\n",
    "Multi-armed bandits (MABs) are a simpler class of problems that capture the fundamental challenge of exploration vs. exploitation in reinforcement learning. The scenario is like a gambler in front of several slot machines (“one-armed bandits”): each pull of a lever gives a stochastic reward. The gambler (agent) wants to maximize reward over time by choosing which machines to play, balancing trying new machines (exploration) and sticking with the best one found so far (exploitation).\n",
    "\n",
    "# Problem Setup\n",
    "- You have K slot machines (arms). Each arm, when pulled, gives a reward drawn from a fixed distribution (unknown to you). For simplicity, let’s say each arm gives a reward of 1 with some probability (and 0 otherwise) – a Bernoulli bandit.\n",
    "- Your goal is to maximize the total reward over a series of pulls (trials).\n",
    "\n",
    "Key challenge: Exploration vs. Exploitation\n",
    "- Exploration: Try different arms to gain information about their payoff.\n",
    "- Exploitation: Use the information to pick the arm with the highest known reward rate so far.\n",
    "\n",
    "\n",
    "We will implement and compare three strategies:\n",
    "1. $\\epsilon$-Greedy – with probability ε, explore (random arm), otherwise exploit (best arm)\n",
    "2. Upper Confidence Bound (UCB) – select the arm with the highest upper confidence bound on reward (favoring arms with less trials so far to explore uncertainty\n",
    "3. Thompson Sampling – a Bayesian approach: maintain a distribution for each arm’s success probability and sample from these to decide (naturally balancing exploration and exploitation)\n",
    "\n",
    "Let’s set up a bandit problem to test these. We will create a bandit with, say, 3 arms:\n",
    "- Arm 0: win probability 0.3\n",
    "- Arm 1: win probability 0.5\n",
    "- Arm 2: win probability 0.7\n",
    "\n",
    "The best arm is #2 (70% chance of reward), but the agents don’t know that initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af49552f-f959-4c9b-8110-112241caa124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# True probabilities for each arm (unknown to agent)\n",
    "true_probs = [0.3, 0.5, 0.7]\n",
    "K = len(true_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f025500-78c0-4667-b6da-5c44374426b6",
   "metadata": {},
   "source": [
    "We’ll simulate a sequence of 1000 pulls. At each step, the agent chooses an arm according to the strategy, and gets a reward 1 (with the arm’s true probability) or 0. We will track:\n",
    "- The cumulative reward over time.\n",
    "- The number of pulls of each arm (to see exploration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275564e-6db8-44c0-9fc4-48c577a340d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
