{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning\n",
    "\n",
    "Temporal-Difference (TD) learning is a powerful model-free approach that blends Monte Carlo and Dynamic Programming ideas. Like MC, TD learns from raw experience without a model. Like DP, TD updates estimates based partly on other learned estimates (bootstrapping). This allows TD to learn online after each step, without waiting for the episode to finish.\n",
    "Key concepts:\n",
    "- TD(0) Prediction: Also known as one-step TD or TD(0), updates the value $V(s)$ toward the observed reward plus the value of the next state:\n",
    "$$V(s) \\leftarrow V(s) + \\alpha [R_{t+1} + \\gamma V(s_{t+1}) - V(s)]$$ \n",
    "This update is based on the TD error $\\delta = R_{t+1} + \\gamma V(s_{t+1}) - V(s)$.\n",
    "\n",
    "\n",
    "- SARSA (on-policy TD control): Updates action-value $Q(s,a)$ toward $R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})$, using the action actually taken next (following current policy)\n",
    "- Q-Learning (off-policy TD control): Updates $Q(s,a)$ toward $R_{t+1} + \\gamma \\max_{a’} Q(s_{t+1}, a’)$, using the greedy best action for the next state (target policy is different from behavior)\n",
    "- On-policy vs Off-policy: SARSA is on-policy (learns value of the policy it follows, including its exploration) whereas Q-learning is off-policy (learns value of the optimal policy independent of the agent’s behavior).\n",
    "\n",
    "Let’s illustrate TD(0) prediction first, then implement SARSA and Q-learning for a control task.\n",
    "\n",
    "### TD(0) Prediction Example\n",
    "\n",
    "We’ll use the gambler’s problem (or we could use grid world) and see how TD updates converge to true values.\n",
    "\n",
    "Let’s do TD(0) for gambler policy “bet 1”, as we did with MC, and see if it approximates the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD(0) estimated value of state 5 (starting state): 0.515\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "def td0_prediction(policy_func, alpha=0.1, episodes=1000, start_capital=5, goal=10, p_head=0.5):\n",
    "    # Initialize value estimates\n",
    "    V = {s: 0.0 for s in range(goal+1)}\n",
    "    V[goal] = 1.0\n",
    "    V[0] = 0.0\n",
    "    for ep in range(episodes):\n",
    "        # start at given start_capital each episode\n",
    "        capital = start_capital\n",
    "        while capital not in (0, goal):\n",
    "            a = policy_func(capital)\n",
    "            next_capital = capital + a if random.random() < p_head else capital - a\n",
    "            reward = 0\n",
    "            # TD update\n",
    "            V[capital] += alpha * (reward + V[next_capital] - V[capital])\n",
    "            capital = next_capital\n",
    "        # Terminal state update (the last transition gives reward 1 or 0)\n",
    "        # Actually, we can handle the terminal step when loop breaks:\n",
    "        if capital == goal:\n",
    "            # If we consider the step into terminal yielding reward 1:\n",
    "            # last state before terminal (call it prev) would have gotten update in loop as:\n",
    "            # V(prev) += alpha * (1 + 0 - V(prev))\n",
    "            # But since we broke out after reaching terminal, let's simulate the last update:\n",
    "            pass\n",
    "    return V\n",
    "\n",
    "td_values = td0_prediction(policy_func=lambda s: 1, alpha=0.1, episodes=10000, start_capital=5, goal=10, p_head=0.5)\n",
    "print(\"TD(0) estimated value of state 5 (starting state):\", round(td_values[5], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Code Explanation\n",
    "\n",
    "- we start with some capital (default 5)\n",
    "- our goal is to reach a certain amount (default 10)\n",
    "- each round you can bet any amount up to your current capital\n",
    "- you have a 50% chance of winning (p_head=0.5)\n",
    "If you win, you get the amount you bet; if you lose, you lose the amount you bet. You want to find the optimal betting strategy\n",
    "\n",
    "We might need to refine handling of the terminal reward. But essentially, TD(0) will update on each step towards the value of the next state. Over many episodes, it should converge to the true $v_{\\pi}(5) \\approx 0.5$. You can try different starting states or do multiple start states in episodes to cover more states.\n",
    "\n",
    "Think of it like a gambler trying to turn $5 into $10 by making a series of bets. The value function $V(s)$ represents the probability of winning (reaching $10) starting from state $s$ under the current policy.\n",
    "\n",
    "TD has an advantage of updating during the episode, and it can learn from incomplete episodes (if we use continuing tasks or bootstrapping without waiting for termination).\n",
    "\n",
    "Now, let’s move to control with SARSA and Q-learning, which are more commonly demonstrated on tasks like Grid World or Cliff Walking.\n",
    "\n",
    "\n",
    "## SARSA vs Q-Learning on Grid World\n",
    "\n",
    "We’ll use the grid world, but introduce a slight twist: let’s add a “bad state” to illustrate the difference between on-policy and off-policy. A classic example is the “Cliff” environment (Sutton & Barto Example 6.6) where following the optimal path has risk of falling off a cliff (big negative reward) vs a safer path.\n",
    "\n",
    "For simplicity, let’s create a small grid where:\n",
    "- Start at (0,0), goal at (0,3).\n",
    "- There’s a “cliff” at positions (1,1) for example that gives a large negative reward if stepped into.\n",
    "- The optimal policy might go around the cliff in an on-policy method vs jump over in off-policy.\n",
    "\n",
    "**SARSA and Q-learning:**\n",
    "- SARSA will update using the action it actually takes next (which, if it’s following $\\epsilon$-greedy, might be suboptimal, so it ends up learning the value of its exploratory policy).\n",
    "- Q-learning will update as if it always takes the optimal next action (even if it didn’t), thus it tends to learn optimistic values and converge to the true optimal policy value.\n",
    "\n",
    "Let’s still do a simple grid with some penalty state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rows, grid_cols = 3, 4\n",
    "start_state = (0,0)\n",
    "goal_state = (0,3)\n",
    "cliff_state = (1,2)  # stepping here gives -10 reward (for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a step function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_step(state, action):\n",
    "    if state == goal_state:\n",
    "        return state, 0  # absorbing\n",
    "    r, c = state\n",
    "    dr, dc = action\n",
    "    nr, nc = r+dr, c+dc\n",
    "    # bounds check\n",
    "    if nr < 0 or nr >= grid_rows or nc < 0 or nc >= grid_cols:\n",
    "        nr, nc = r, c  # hit wall, stay (could also give a small neg reward)\n",
    "    new_state = (nr, nc)\n",
    "    # reward logic\n",
    "    if new_state == cliff_state:\n",
    "        reward = -10\n",
    "    elif new_state == goal_state:\n",
    "        reward = +10\n",
    "    else:\n",
    "        reward = -1  # each step cost\n",
    "    return new_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define actions and run SARSA and Q-learning episodes.\n",
    "\n",
    "We’ll keep it short and not do too many episodes for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy policy from SARSA:\n",
      "< > > G\n",
      "^ ^ ^ ^\n",
      "^ ^ < <\n",
      "\n",
      "Greedy policy from Q-learning:\n",
      "> > > G\n",
      "^ ^ ^ ^\n",
      "v > < ^\n"
     ]
    }
   ],
   "source": [
    "actions = [(0,1),(0,-1),(1,0),(-1,0)]  # right, left, down, up moves\n",
    "def epsilon_greedy_action(Q, state, epsilon=0.1):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        # choose best action from Q\n",
    "        q_values = [Q.get((state,a), 0) for a in actions]\n",
    "        max_q = max(q_values)\n",
    "        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "def run_sarsa(alpha=0.5, gamma=1.0, epsilon=0.1, episodes=1000):\n",
    "    Q = {}\n",
    "    for ep in range(episodes):\n",
    "        state = start_state\n",
    "        action = epsilon_greedy_action(Q, state, epsilon)\n",
    "        while state != goal_state:\n",
    "            next_state, reward = grid_step(state, action)\n",
    "            next_action = epsilon_greedy_action(Q, next_state, epsilon)\n",
    "            # Q(s,a) update\n",
    "            current_q = Q.get((state, action), 0.0)\n",
    "            next_q = Q.get((next_state, next_action), 0.0)\n",
    "            td_target = reward + gamma * next_q\n",
    "            Q[(state, action)] = current_q + alpha * (td_target - current_q)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    return Q\n",
    "\n",
    "def run_q_learning(alpha=0.5, gamma=1.0, epsilon=0.1, episodes=1000):\n",
    "    Q = {}\n",
    "    for ep in range(episodes):\n",
    "        state = start_state\n",
    "        while state != goal_state:\n",
    "            action = epsilon_greedy_action(Q, state, epsilon)\n",
    "            next_state, reward = grid_step(state, action)\n",
    "            # Q-learning update\n",
    "            current_q = Q.get((state, action), 0.0)\n",
    "            # next state's best action value\n",
    "            next_q_values = [Q.get((next_state,a), 0.0) for a in actions]\n",
    "            next_max = max(next_q_values) if next_q_values else 0.0\n",
    "            td_target = reward + gamma * next_max\n",
    "            Q[(state, action)] = current_q + alpha * (td_target - current_q)\n",
    "            state = next_state\n",
    "    return Q\n",
    "\n",
    "# Run SARSA and Q-learning\n",
    "Q_sarsa = run_sarsa(episodes=1000)\n",
    "Q_qlearn = run_q_learning(episodes=1000)\n",
    "\n",
    "# Derive policies from Q (greedy)\n",
    "policy_sarsa = {}\n",
    "policy_qlearn = {}\n",
    "for r in range(grid_rows):\n",
    "    for c in range(grid_cols):\n",
    "        s = (r,c)\n",
    "        # skip goal\n",
    "        if s == goal_state:\n",
    "            continue\n",
    "        # find best action in Q for state\n",
    "        q_vals_sarsa = {a: Q_sarsa.get((s,a), 0.0) for a in actions}\n",
    "        q_vals_q = {a: Q_qlearn.get((s,a), 0.0) for a in actions}\n",
    "        best_a_sarsa = max(q_vals_sarsa, key=q_vals_sarsa.get)\n",
    "        best_a_q = max(q_vals_q, key=q_vals_q.get)\n",
    "        policy_sarsa[s] = best_a_sarsa\n",
    "        policy_qlearn[s] = best_a_q\n",
    "\n",
    "print(\"Greedy policy from SARSA:\")\n",
    "for r in range(grid_rows):\n",
    "    row = []\n",
    "    for c in range(grid_cols):\n",
    "        s = (r,c)\n",
    "        if s == goal_state:\n",
    "            row.append(\"G\")\n",
    "        elif policy_sarsa.get(s) is None:\n",
    "            row.append(\".\")\n",
    "        else:\n",
    "            arrow = { (0,1): \">\", (0,-1): \"<\", (1,0): \"v\", (-1,0): \"^\" }\n",
    "            row.append(arrow[policy_sarsa[s]])\n",
    "    print(\" \".join(row))\n",
    "\n",
    "print(\"\\nGreedy policy from Q-learning:\")\n",
    "for r in range(grid_rows):\n",
    "    row = []\n",
    "    for c in range(grid_cols):\n",
    "        s = (r,c)\n",
    "        if s == goal_state:\n",
    "            row.append(\"G\")\n",
    "        elif policy_qlearn.get(s) is None:\n",
    "            row.append(\".\")\n",
    "        else:\n",
    "            arrow = { (0,1): \">\", (0,-1): \"<\", (1,0): \"v\", (-1,0): \"^\" }\n",
    "            row.append(arrow[policy_qlearn[s]])\n",
    "    print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
