{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa11f56c-0f49-4fd2-9f85-02b14e23bf81",
   "metadata": {},
   "source": [
    "#  Dynamic Programming\n",
    "\n",
    "Dynamic Programming (DP) methods solve reinforcement learning problems when we have a perfect model of the environment (known states, actions, transition probabilities, and rewards). DP is essentially planning: using Bellman equations to compute value functions and optimal policies. While not used directly in model-free learning, DP lays the groundwork for understanding value functions and optimality.\n",
    "\n",
    "\n",
    "Key ideas:\n",
    "- Policy Evaluation: Compute the value function $v_{\\pi}(s)$ for a given policy π (prediction with a model).\n",
    "- Policy Improvement: Given value function $v_{\\pi}$, improve the policy by acting greedily w.r.t. those values.\n",
    "- Policy Iteration: Iteratively alternate evaluation and improvement until convergence to an optimal policy\n",
    "- Value Iteration: A streamlined approach that combines evaluation and improvement in one step (update values towards optimality directly)\n",
    "  \n",
    "We'll demonstrate these in a simple Grid World environment.\n",
    "\n",
    "## Grid World Environment\n",
    "Consider a 4x4 grid (like a little board game). The agent starts in some cell and can move up, down, left, or right:\n",
    "- Some cells are terminal states (once reached, the episode ends).\n",
    "- For simplicity, let's use the top-left (0,0) and bottom-right (3,3) as terminal states. The goal could be to reach the bottom-right corner.\n",
    "- Each step gives a reward of -1 (so there's a penalty per move, which encourages finding the shortest path to a terminal).\n",
    "\n",
    "This setup is similar to the example in Sutton & Barto (Gridworld), where the objective is to reach a goal in as few steps as possible (minimize cost).\n",
    "\n",
    "Let's define the environment dynamics:\n",
    "- States: (i,j) grid positions, i,j ∈ {0,1,2,3}. Two terminal states: (0,0) and (3,3).\n",
    "- Actions: up, down, left, right.\n",
    "- Transition: deterministic moves, but if an action would take the agent off-grid, it remains in the same state.\n",
    "- Reward: -1 for each action (except maybe 0 at terminal since episode ends there).\n",
    "- Discount factor γ = 1 (we'll treat it as an episodic task with finite horizon implicitly, focusing on minimizing steps).\n",
    "\n",
    "We'll represent the value function as a 4x4 matrix for convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0bb3ae-ab6d-48a1-9f24-ace30e7b8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define grid world parameters\n",
    "rows = cols = 4\n",
    "terminal_states = [(0,0), (3,3)]\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right as changes in (row, col)\n",
    "\n",
    "def is_terminal(state):\n",
    "    return state in terminal_states\n",
    "\n",
    "# Helper to get next state and reward\n",
    "def step(state, action):\n",
    "    if is_terminal(state):\n",
    "        return state, 0  # no change if terminal\n",
    "    r, c = state\n",
    "    dr, dc = action\n",
    "    new_r, new_c = r + dr, c + dc\n",
    "    # if out of bounds, stay in same state\n",
    "    if new_r < 0 or new_r >= rows or new_c < 0 or new_c >= cols:\n",
    "        new_r, new_c = r, c\n",
    "    new_state = (new_r, new_c)\n",
    "    reward = 0 if is_terminal(new_state) else -1\n",
    "    return new_state, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dee992-d399-4d4a-8418-e21fdd4d287b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
